{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS WORKSHEET-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 to Q10 are MCQs with only one correct answer. Choose the correct option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using a goodness of fit,we can assess whether a set of obtained frequencies differ from a set of frequencies.\n",
    "\n",
    "a) Mean\n",
    "b) Actual\n",
    "c) Predicted\n",
    "d) Expected\n",
    "\n",
    "--> d) Expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chisquare is used to analyse\n",
    "\n",
    "a) Score\n",
    "b) Rank\n",
    "c) Frequencies\n",
    "d) All of these\n",
    "\n",
    "--> c) Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the mean of a Chi Square distribution with 6 degrees of freedom?\n",
    "\n",
    "a) 4\n",
    "b) 12\n",
    "c) 6\n",
    "d) 8\n",
    "\n",
    "--> c) 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Which of these distributions is used for a goodness of fit testing?\n",
    "\n",
    "a) Normal distribution\n",
    "b) Chisqared distribution\n",
    "c) Gamma distribution\n",
    "d) Poission distribution\n",
    "\n",
    "--> b) Chisqared distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which of the following distributions is Continuous\n",
    "\n",
    "a) Binomial Distribution\n",
    "b) Hypergeometric Distribution\n",
    "c) F Distribution\n",
    "d) Poisson Distribution\n",
    "\n",
    "--> c) F Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. A statement made about a population for testing purpose is called?\n",
    "\n",
    "a) Statistic\n",
    "b) Hypothesis\n",
    "c) Level of Significance\n",
    "d) TestStatistic\n",
    "\n",
    "--> b) Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. If the assumed hypothesis is tested for rejection considering it to be true is called?\n",
    "\n",
    "a) Null Hypothesis\n",
    "b) Statistical Hypothesis\n",
    "c) Simple Hypothesis\n",
    "d) Composite Hypothesis\n",
    "\n",
    "--> a) Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. If the Critical region is evenly distributed then the test is referred as?\n",
    "\n",
    "a) Two tailed\n",
    "b) One tailed\n",
    "c) Three tailed\n",
    "d) Zero tailed\n",
    "\n",
    "--> a) Two tailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Alternative Hypothesis is also called as?\n",
    "\n",
    "a) Composite hypothesis\n",
    "b) Research Hypothesis\n",
    "c) Simple Hypothesis\n",
    "d) Null Hypothesis\n",
    "\n",
    "--> b) Research Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. In a Binomial Distribution, if ‘n’ is the number of trials and ‘p’ is the probability of success, then the mean value is \n",
    "given by\n",
    "\n",
    "a) np\n",
    "b) n\n",
    "\n",
    "--> a) np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSHEET 5 SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write SQL query to show all the data in the Movie table.\n",
    "\n",
    "--> select * from movie;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write SQL query to show the title of the longest runtime movie.\n",
    "\n",
    "--> select title from movie where runtime=(select runtime from movie group by runtime order by desc limit 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write SQL query to show the highest revenue generating movie title.\n",
    "\n",
    "--> select title from movie where revenue=(select revenue from movie group by revenue order by desc limit 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write SQL query to show the movie title with maximum value of revenue/budget.\n",
    "\n",
    "--> select title from movie where revenue=(select revenue from movie group by revenue order by desc limit 1) or budget=(select budget order by desc limit 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a SQL query to show the movie title and its cast details like name of the person, gender, character \n",
    "name, cast order.\n",
    "\n",
    "--> select title, person_name, gender, character_name, cast_order from movie join movie_caast using(movie_id) join person using(person_id) join gender(gender_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a SQL query to show the country name where maximum number of movies has been produced, along \n",
    "with the number of movies produced.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a SQL query to show all the genre_id in one column and genre_name in second column.\n",
    "\n",
    "--> select genre_id, genre_name from genre;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a SQL query to show name of all the languages in one column and number of movies in that \n",
    "particular column in another column.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write a SQL query to show movie name in first column, no. of crew members in second column and \n",
    "number of cast members in third column.\n",
    "\n",
    "--> select title, count(movie_id), count(movie_id) from movie inner join movie_crew using(movie_id) inner join movie_cast using(movie_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Write a SQL query to list top 10 movies title according to popularity column in decreasing order.\n",
    "\n",
    "--> select title from movie where popularity=(select popularity from movie group by popularity order by desc limit 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Write a SQL query to show the name of the 3rd most revenue generating movie and its revenue.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Write a SQL query to show the names of all the movies which have “rumoured” movie status.\n",
    "\n",
    "--> select title from movie where movie_status=\"rumoured\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Write a SQL query to show the name of the “United States of America” produced movie which generated \n",
    "maximum revenue.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write a SQL query to print the movie_id in one column and name of the production company in the second \n",
    "column for all the movies.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Write a SQL query to show the title of top 20 movies arranged in decreasing order of their budget.\n",
    "\n",
    "--> select title from movie where budget=(select budget from movie group by budget order by desc limit 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 to Q15 are subjective answer type questions, Answer them briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. R-squared or Residual Sum of Squares (RSS) which one of these two is a better measure of \n",
    "goodness of fit model in regression and why?\n",
    "\n",
    "-->Residual Sum of Squares is a better measure of goodness of fit model in regression.\n",
    "\n",
    "A residual sum of squares (RSS) measures the level of variance in the error term, or residuals, of a regression model.\n",
    "\n",
    "Ideally, the sum of squared residuals should be a smaller or lower value than the sum of squares from the regression model's inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are TSS (Total Sum of Squares), ESS (Explained Sum of Squares) and RSS (Residual Sum \n",
    "of Squares) in regression. Also mention the equation relating these three metrics with each other.\n",
    "\n",
    "--> The Total SS (TSS or SST) tells you how much variation there is in the dependent variable.\n",
    "\n",
    "The Explained SS tells you how much of the variation in the dependent variable your model explained.\n",
    "\n",
    "The residual sum of squares tells you how much of the dependent variable’s variation your model did not explain. It is the sum of the squared differences between the actual Y and the predicted Y.\n",
    "\n",
    "TSS = ESS + RSS, where TSS is Total Sum of Squares, ESS is Explained Sum of Squares and RSS is Residual Sum of Suqares. The aim of Regression Analysis is explain the variation of dependent variable Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the need of regularization in machine learning?\n",
    "\n",
    "--> This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is Gini–impurity index? \n",
    "\n",
    "--> Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. But what is actually meant by 'impurity'? If all the elements belong to a single class, then it can be called pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Are unregularized decision-trees prone to overfitting? If yes, why? \n",
    "\n",
    "--> unregularized decision trees are prone to overfitting because when we don't do regularization our model tends to learn the noise from the data and adjusts to all the complex outcomes which may affect our unseen data.\n",
    "\n",
    "In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus it ends up with branches with strict rules of sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is an ensemble technique in machine learning?\n",
    "\n",
    "--> Ensemble methods are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. This has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is the difference between Bagging and Boosting techniques?\n",
    "\n",
    "--> Bagging is a method of merging the same type of predictions. Boosting is a method of merging different types of predictions. Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is out-of-bag error in random forests? \n",
    "\n",
    "--> Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging). Bagging uses subsampling with replacement to create training samples for the model to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is K-fold cross-validation?\n",
    "\n",
    "--> In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What is hyper parameter tuning in machine learning and why it is done?\n",
    "\n",
    "--> In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
    "\n",
    "Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained.\n",
    "\n",
    "“A good choice of hyperparameters can really make an algorithm shine”.\n",
    "\n",
    "Choosing appropriate hyperparameters plays a crucial role in the success of our neural network architecture. Since it makes a huge impact on the learned model. For example, if the learning rate is too low, the model will miss the important patterns in the data. If it is high, it may have collisions.\n",
    "\n",
    "Choosing good hyperparameters gives two benefits:\n",
    "Efficiently search the space of possible hyperparameters\n",
    "Easy to manage a large set of experiments for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What issues can occur if we have a large learning rate in Gradient Descent? \n",
    "\n",
    "--> At extremes, a learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. Oscillating performance is said to be caused by weights that diverge (are divergent). A learning rate that is too small may never converge or may get stuck on a suboptimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Can we use Logistic Regression for classification of Non-Linear Data? If not, why?\n",
    "\n",
    "--> Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space to separate observations that belong to a class from all the other observations that do not belong to that class. The decision boundary is thus linear. Robust and efficient implementations are readily available (e.g. scikit-learn) to use logistic regression as a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Differentiate between Adaboost and Gradient Boosting.\n",
    "\n",
    "--> AdaBoost is the first designed boosting algorithm with a particular loss function. On the other hand, Gradient Boosting is a generic algorithm that assists in searching the approximate solutions to the additive modelling problem. This makes Gradient Boosting more flexible than AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. What is bias-variance trade off in machine learning?\n",
    "\n",
    "--> Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Give short description each of Linear, RBF, Polynomial kernels used in SVM.\n",
    "\n",
    "--> Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line. It is one of the most common kernels to be used. It is mostly used when there are a Large number of Features in a particular Data Set. One of the examples where there are a lot of features, is Text Classification, as each alphabet is a new feature. So we mostly use Linear Kernel in Text Classification.\n",
    "\n",
    "When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\n",
    "\n",
    "In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
