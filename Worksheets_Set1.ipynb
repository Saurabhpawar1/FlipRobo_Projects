{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS WORKSHEET-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 to Q9 have only one correct answer. Choose the correct option to answer your question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Bernoulli random variables take (only) the values 1 and 0.\n",
    "    a) True\n",
    "    b) False\n",
    "\n",
    "--> a) True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Which of the following theorem states that the distribution of averages of iid variables, properly normalized, becomes that of a standard normal as the sample size increases?\n",
    "    a) Central Limit Theorem\n",
    "    b) Central Mean Theorem\n",
    "    c) Centroid Limit Theorem\n",
    "    d) All of the mentioned\n",
    "\n",
    "--> a) Central Limit Theorem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Which of the following is incorrect with respect to use of Poisson distribution?\n",
    "    a) Modeling event/time data\n",
    "    b) Modeling bounded count data\n",
    "    c) Modeling contingency tables\n",
    "    d) All of the mentioned\n",
    "\n",
    "--> b) Modeling bounded count data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Point out the correct statement.\n",
    "    a) The exponent of a normally distributed random variables follows what is called the log- normal distribution\n",
    "    b) Sums of normally distributed random variables are again normally distributed even if the variables are dependent\n",
    "    c) The square of a standard normal random variable follows what is called chi-squared distribution\n",
    "    d) All of the mentioned\n",
    "\n",
    "--> d) All of the mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. ______ random variables are used to model rates.\n",
    "    a) Empirical\n",
    "    b) Binomial\n",
    "    c) Poisson\n",
    "    d) All of the mentioned\n",
    "\n",
    "--> c) Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Usually replacing the standard error by its estimated value does change the CLT.\n",
    "    a) True\n",
    "    b) False\n",
    "\n",
    "--> b) False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Which of the following testing is concerned with making decisions using data?\n",
    "    a) Probability\n",
    "    b) Hypothesis\n",
    "    c) Causal\n",
    "    d) None of the mentioned\n",
    "\n",
    "--> b) Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Normalized data are centered at______and have units equal to standard deviations of the original data.\n",
    "    a) 0\n",
    "    b) 5\n",
    "    c) 1\n",
    "    d) 10\n",
    "\n",
    "--> a) 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    9. Which of the following statement is incorrect with respect to outliers?\n",
    "    a) Outliers can have varying degrees of influence\n",
    "    b) Outliers can be the result of spurious or real processes\n",
    "    c) Outliers cannot conform to the regression relationship\n",
    "    d) None of the mentioned\n",
    "\n",
    "--> c) Outliers cannot conform to the regression relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10 to Q15 are subjective answer type questions, Answer them in your own words briefly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    10. What do you understand by the term Normal Distribution?\n",
    "\n",
    "-->\n",
    "\n",
    "    1. Normal distribution is also known as Gaussian Distribution.\n",
    "\n",
    "    2. Normal distribution is a probability distribution that is symmetric about the mean i.e showing that data near the mean are more frequent in occurrence than data far from the mean.\n",
    "    \n",
    "    3. In graph form, normal distribution will appear as a bell curve.\n",
    "    \n",
    "    4. The normal distribution is the most common type of distribution assumed in statistical analyses. However Real life data rarely follows a perfect normal distribution.\n",
    "    \n",
    "    5. The standard normal distribution has two parameters: the mean and the standard deviation. For a normal distribution, 68% of the observations are within +/- one standard deviation of the mean, 95% are within +/- two standard deviations, and 99.7% are within +- three standard deviations.\n",
    "    \n",
    "    6. The skewness and kurtosis coefficients measure how different a given distribution is from a normal distribution.\n",
    "    \n",
    "    7. The skewness measures the symmetry of a distribution. The normal distribution is symmetric and has a skewness of zero. If the distribution of a data set has a skewness less than zero, or negative skewness, then the left tail of the distribution is longer than the right tail. positive skewness implies that the right tail of the distribution is longer than the left.\n",
    "    \n",
    "    8. The kurtosis statistic measures the thickness of the tail ends of a distribution in relation to the tails of the normal distribution.\n",
    "    \n",
    "    9. The normal distribution model is motivated by the Central Limit Theorem.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    11. How do you handle missing data? What imputation techniques do you recommend?\n",
    "    \n",
    "-->    \n",
    "\n",
    "\n",
    "    1. Firstly I will interpret that why there are missing values in our dataset. For instance some people hesitate to put down the information for eample Men hesitate to put down the salary and Women hesitate to put down the age, Survey informations are not that valid, People may have died.\n",
    "    \n",
    "    2. Then I will interpret what are the different types of missing data:\n",
    "    \n",
    "        i) Missing at Random (MAR):\n",
    "        Missing at random means that the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.\n",
    "        \n",
    "        ii) Missing Completely at Random (MCAR): \n",
    "        The fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.\n",
    "        \n",
    "        iii) Missing not at Random (MNAR):\n",
    "        Two possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable’s value (e.g. Let’s assume that females generally don’t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)\n",
    "        \n",
    "    3. In the first two cases, it is safe to remove the data with missing values depending upon their occurrences, while in the third case removing observations with missing values can produce a bias in the model.\n",
    "    \n",
    "    4. Very basic type of imputation technique is Computing the overall mean, it is the only tested function that takes no advantage of the time series characteristics or relationship between the variables. It is very fast, but has clear disadvantages. One disadvantage is that mean imputation reduces variance in the dataset.\n",
    "    \n",
    "    5. There are other machine learning techniques like XGBoost and Random Forest for data imputation but personally my favorite imputation technique is KNN(K Nearest Neighbors) and alos it is widely used. In this method, k neighbors are chosen based on some distance measure and their average is used as an imputation estimate. The method requires the selection of the number of nearest neighbors, and a distance metric. KNN can predict both discrete attributes (the most frequent value among the k nearest neighbors) and continuous attributes (the mean among the k nearest neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    12. What is A/B testing?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. A/B testing is basically used to compare two different products those comparison is made from the user input, whether the user is clicking on product 1 or user is clicking on product 2.\n",
    "    \n",
    "    2. Technically we can say that A/B testing is a basic randomized control experiment. It is a way to compare the two versions of a variable to find out which performs better in a controlled environment.\n",
    "    \n",
    "    3. For instance, let’s say you own a company and want to increase the sales of your product. Here, either you can use random experiments, or you can apply scientific and statistical methods. A/B testing is one of the most prominent and widely used statistical tools.\n",
    "    \n",
    "    4. In the above scenario, you may divide the products into two parts – A and B. Here A will remain unchanged while you make significant changes in B’s packaging. Now, on the basis of the response from customer groups who used A and B respectively, you try to decide which is performing better.\n",
    "\n",
    "    5. It is a hypothetical testing methodology for making decisions that estimate population parameters based on sample statistics. The population refers to all the customers buying your product, while the sample refers to the number of customers that participated in the test.\n",
    "    \n",
    "    6. A/B testing works best when testing incremental changes, such as UX changes, new features, ranking, and page load times. Here you may compare pre and post-modification results to decide whether the changes are working as desired or not.\n",
    "    \n",
    "    7. A/B testing doesn’t work well when testing major changes, like new products, new branding, or completely new user experiences. In these cases, there may be effects that drive higher than normal engagement or emotional responses that may cause users to behave in a different manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    13. Is mean imputation of missing data acceptable practice?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. Mean imputation is a very basic type of imputation technique.\n",
    "    \n",
    "    2. It is the only tested function that takes no advantage of the time series characteristics or relationship between the variables.\n",
    "    \n",
    "    3. Mean imputation reduces the variance of the imputed variables.\n",
    "    \n",
    "    4. Mean imputation shrinks standard errors, which invalidates most hypothesis tests and the calculation of confidence interval.\n",
    "    \n",
    "    5. Mean imputation does not preserve relationships between variables such as correlations.\n",
    "    \n",
    "    6. Thus for a professional Data Scientist mean imputation of missing data is not at all acceptable practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    14. What is linear regression in statistics?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. Linear regression is used to study the linear relationship between a dependent variable Y (blood pressure) and one or more independent variables X (age, weight, sex).\n",
    "    \n",
    "    2. More genreally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term, also called the intercept term.\n",
    "    \n",
    "    3. There are two types of linear regression one is simple linear regression and other is multiple lineaar regeression.\n",
    "    \n",
    "    4. In simple linear regression, we predict scores on one variable from the scores on a second variable. The variable we are predicting is called the criterion variable and is referred to as  Y . The variable we are basing our predictions on is called the predictor variable and is referred to as  X .\n",
    "    \n",
    "    5. When there is only one predictor variable, the prediction method is called simple regression. In simple linear regression, the predictions of  Y  when plotted as a function of  X  form a straight line.\n",
    "    \n",
    "    6. When there are multiple predictor variable, the prediction method is called multiple linear regresson. The statistcal formaula for multiple linear regresson is  y = 0 + 1x1 + 2x2 + ... + pxp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    15. What are the various branches of statistics?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    There are two branches of statistics they are:-\n",
    "    i) Descriptive statistics\n",
    "    ii) Inferential statistics\n",
    "    \n",
    "    Descriptive statistics:-\n",
    "    \n",
    "    It organizes raw data into meaningful information.An house hold articles manufacturing company would like to know what people feel about their products. For that purpose, the company forms a team of people and tries to collect information from the public. The team of people formed by the company is trying to collect data from the public directly. The data which is being collected directly from the public will always not be meaning full. Hence, the data which is being collected directly from the public has to be converted in to meaningful information. This is the work being done in this particular branch “ descriptive-statistics\". That is,it focuses on collecting, summarizing and presenting set of data.\n",
    "    \n",
    "    For example, Industrial statistics, population statistics, trade statistics etc.\n",
    "    \n",
    "    Inferential statistics:-\n",
    "    \n",
    "    It analyses sample data to draw conclusion about population. Marketing research team of a company wants to know how far the people need a particular product manufactured by the company. There is one hundred thousand population in a particular city. It is bit difficult to go and ask all one hundred thousand people, due to time consumption and other factors. Hence, it takes a sample of 1000 people to draw conclusion for the whole population. That is making general statement from the study of particular cases or any treatment of data, which leads to prediction or inference concerning a larger group of data.\n",
    "    \n",
    "    For example, we want to have an idea about percentage of illiterates in a country. We take a sample from a population and the proportion of illiterates in the sample. That sample with the help of probability enables us to find the proportion to the original population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL WORKSHEET-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 and Q2 have one or more correct answer. Choose all the correct option to answer your question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Which of the following is/are DDL commands in SQL?\n",
    "    A) Create B) Update\n",
    "    C) Delete D) ALTER\n",
    "\n",
    "--> A) Create\n",
    "    D) ALTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Which of the following is/are DML commands in SQL?\n",
    "    A) Update B) Delete\n",
    "    C) Select D) Drop\n",
    "\n",
    "--> A) Update\n",
    "    B) Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 to Q10 have only one correct answer. Choose the correct option to answer your question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Full form of SQL is:\n",
    "    A) Strut querying language B) Structured Query Language\n",
    "    C) Simple Query Language D) None of them\n",
    "\n",
    "--> B) Structured Query Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Full form of DDL is:\n",
    "    A) Descriptive Designed Language B) Data Definition Language\n",
    "    C) Data Descriptive Language D) None of the above.\n",
    "\n",
    "--> B) Data Defination Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. DML is:\n",
    "    A) Data Manipulation Language B) Data Management Language\n",
    "    C) Data Modeling Language D) None of these\n",
    "\n",
    "--> A) Data Manipualtion Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Which of the following statements can be used to create a table with column B int type and C floattype?\n",
    "    A) Table A (B int, C float) B) Create A (b int, C float)\n",
    "    C) Create Table A (B int,C float) D) All of them\n",
    "\n",
    "--> C) Create Table A (B int, C float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. Which of the following statements can be used to add a column D (float type) to the table A created above?\n",
    "    A) Table A ( D float) B) Alter Table A ADD COLUMN D float\n",
    "    C) Table A( B int, C float, D float) D) None of them\n",
    "\n",
    "--> B) Alter Table A ADD COLUMN D float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Which of the following statements can be used to drop the column added in the above question?\n",
    "    A) Table A Drop D B) Alter Table A Drop Column D\n",
    "    C) Delete D from A D) None of them\n",
    "\n",
    "--> B) Alter Table A Drop Column D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    9. Which of the following statements can be used to change the data type (from float to int ) of the column Dof table A created in above questions?\n",
    "    A) Table A (D float int) B) Alter Table A Alter Column D int\n",
    "    C) Alter Table A D float int D) Alter table A Column D float to int\n",
    "\n",
    "--> B) Alter Table A Alter Column D int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    10. Suppose we want to make Column B of Table A as primary key of the table. By which of the following statements we can do it?\n",
    "    A) Alter Table A Add Constraint Primary Key B B) Alter table (B primary key)\n",
    "    C) Alter Table A Add Primary key B D) None of them\n",
    "\n",
    "--> C) Alter Table A Add Primary key B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11 to Q15 are subjective answer type questions, Answer them briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    11. What is data-warehouse?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. A data warehouse is a large collection of business data used to help an organization make decisions.\n",
    "    \n",
    "    2. One of the primary components in a SQL Server business intelligence (BI) solution is the data warehouse.\n",
    "    \n",
    "    3. Indeed, the data warehouse is, in a sense, the glue that holds the system together. The warehouse acts as a central repository for heterogeneous data that is to be used for purposes of analysis and reporting.\n",
    "    \n",
    "    4. A data warehouse consolidates, standardizes, and organizes data in order to support business decisions that are made through analysis and reporting.\n",
    "    \n",
    "    5. The data might originate in RDBMSs such as SQL Server or Oracle, Excel spreadsheets, CSV files, directory services stores such as Active Directory, or other types of data stores, as is often the case in large enterprise networks.\n",
    "    \n",
    "    6. The data warehouse must be able to store data from a variety of data sources in a way that lets tools such as SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) efficiently access the data. These tools are, in effect, indifferent to the original data sources and are concerned only with the reliability and viability of the data in the warehouse.\n",
    "    \n",
    "    7. A data warehouse is sometimes considered to be a place for archiving data; however, that is not its purpose. Although historical data is stored in a data warehouse, only the historical range necessary to support analysis and reporting is retained there.\n",
    "    \n",
    "    8. For example, if a business rule specifies that the warehouse must maintain two years worth of historical data, older data is offloaded to another system for archiving or is deleted, depending on the specified business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    12. What is the difference between OLTP VS OLAP?\n",
    "    \n",
    "-->\n",
    "\n",
    "    \n",
    "    1. OLTP is an acronym for Online transaction processing. OLAP is an acronym for Online analytical processing.\n",
    "    \n",
    "    2. OLTP is an online transactional system. It manages database modification. OLAP is an online analysis and data retrieving process.\n",
    "    \n",
    "    3. OLTP is characterized by large numbers of short online transactions. OLAP is characterized by a large volume of data.\n",
    "    \n",
    "    4. OLTP is an online database modifying system. OLAP is an online database query management system.\n",
    "    \n",
    "    5. OLTP uses traditional DBMS. OLAP uses the data warehouse.\n",
    "    \n",
    "    6. Tables in OLTP database are normalized. Tables in OLAP database are not normalized.\n",
    "    \n",
    "    7. OLTP and its transactions are the sources of data. Different OLTP databases become the source of data for OLAP.\n",
    "    \n",
    "    8. OLTP database must maintain data integrity constraint. OLAP database does not get frequently modified. Hence, data integrity is not an issue.\n",
    "    \n",
    "    9. OLTP response time is in milliseconds. OLAP response time is in seconds to minutes.\n",
    "    \n",
    "    10. The data in the OLTP database is always detailed and organized. The data in OLAP process might not be organized.\n",
    "    \n",
    "    11. Queries in OLTP process are standardized and simple. OLAP consists Complex queries involving aggregations.\n",
    "    \n",
    "    12. OLTP needs Complete backup of the data combined with incremental backups. OLAP only need a backup from time to time. Backup is not important compared to OLTP.\n",
    "    \n",
    "    13. OLTP is used by Data critical users like clerk, DBA & Data Base professionals. OLAP is Used by Data knowledge users like workers, managers, and CEO.\n",
    "    \n",
    "    14. OLTP is designed for real time business operations. OLAP is designed for analysis of business measures by category and attributes.\n",
    "    \n",
    "    15. OLTP kind of Database users allows thousands of users. OLAP kind of Database allows only hundreds of users.\n",
    "    \n",
    "    16. OLTP is designed to have fast response time, low data redundancy and is normalized. In OLAP a data warehouse is created uniquely so that it can integrate different data sources for building a consolidated database.\n",
    "    \n",
    "    17. An example of OLTP system is ATM center. Any Datawarehouse system is an OLAP system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    13. What are the various characteristics of data-warehouse?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    Here is the list of some of the characteristics of data warehousing:\n",
    "\n",
    "    1. Subject oriented:-\n",
    "    \n",
    "    A data warehouse is subject-oriented, as it provides information on a topic rather than the ongoing operations of organizations. Such issues may be inventory, promotion, storage, etc. Never does a data warehouse concentrate on the current processes. Instead, it emphasized modeling and analyzing decision-making data. It also provides a simple and succinct description of the particular subject by excluding details that would not be useful in helping the decision process.\n",
    "       \n",
    "    2. Integrated:-\n",
    "    \n",
    "    Integration in Data Warehouse means establishing a standard unit of measurement from the different databases for all the similar data. The data must also get stored in a simple and universally acceptable manner within the Data Warehouse. Through combining data from various sources such as a mainframe, relational databases, flat files, etc., a data warehouse is created. It must also keep the naming conventions, format, and coding consistent. Such an application assists in robust data analysis. Consistency must be maintained in naming conventions, measurements of characteristics, specification of encoding, etc.\n",
    "    \n",
    "    3. Time-variant:-\n",
    "    \n",
    "    Compared to operating systems, the time horizon for the data warehouse is quite extensive. The data collected in a data warehouse is acknowledged over a given period and provides historical information. It contains a temporal element, either explicitly or implicitly.\n",
    "\n",
    "    One such location in the record key system where Data Warehouse data shows time variation is. Each primary key contained with the DW should have an element of time either implicitly or explicitly. Just like the day, the month of the week, etc.\n",
    "    \n",
    "    4. Non-volatile:-\n",
    "    \n",
    "    Also, the data warehouse is non-volatile, meaning that prior data will not be erased when new data are entered into it. Data is read-only, only updated regularly. It also assists in analyzing historical data and in understanding what and when it happened. The transaction process, recovery, and competitiveness control mechanisms are not required. In the Data Warehouse environment, activities such as deleting, updating, and inserting that are performed in an operational application environment are omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    14. What is Star-Schema?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. Star schema is the fundamental schema among the data mart schema and it is simplest.\n",
    "    \n",
    "    2. This schema is widely used to develop or build a data warehouse and dimensional data marts. It includes one or more fact tables indexing any number of dimensional tables.\n",
    "    \n",
    "    3. The star schema is a necessary case of the snowflake schema. It is also efficient for handling basic queries.\n",
    "    \n",
    "    4. It is said to be star as its physical model resembles to the star shape having a fact table at its center and the dimension tables at its peripheral representing the star’s points.\n",
    "    \n",
    "    5. In Star Schema, Business process data, that holds the quantitative data about a business is distributed in fact tables, and dimensions which are descriptive characteristics related to fact data.\n",
    "    \n",
    "    6. Often, A Star Schema having multiple dimensions is termed as Centipede Schema. It is easy to handle a star schema which have dimensions of few attributes.\n",
    "    \n",
    "    7. Advantages of Star Schema :-\n",
    "\n",
    "    i) Simpler Queries:\n",
    "    \n",
    "    Join logic of star schema is quite cinch in compare to other join logic which are needed to fetch data from a transactional schema that is highly normalized.\n",
    "    \n",
    "    ii) Simplified Business Reporting Logic:\n",
    "    \n",
    "    In compared to a transactional schema that is highly normalized, the star schema makes simpler common business reporting logic, such as as-of reporting and period-over-period.\n",
    "    \n",
    "    iii) Feeding Cubes:\n",
    "    \n",
    "    Star schema is widely used by all OLAP systems to design OLAP cubes efficiently. In fact, major OLAP systems deliver a ROLAP mode of operation which can use a star schema as a source without designing a cube structure.\n",
    "    \n",
    "    8. Disadvantages of Star Schema :-\n",
    "\n",
    "    i) Data integrity is not enforced well since in a highly de-normalized schema state.\n",
    "    \n",
    "    ii) Not flexible in terms if analytical needs as a normalized data model.\n",
    "    \n",
    "    iii) Star schemas don’t reinforce many-to-many relationships within business entities – at least not frequently.\n",
    "    \n",
    "    9. Sales price, sale quantity, distant, speed, weight, and weight measurements are few examples of fact data in star schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    15. What do you mean by SETL?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. SETL (SET Language) is a very high-level programming language based on the mathematical theory of sets.\n",
    "    \n",
    "    2. It was originally developed by (Jack) Jacob T. Schwartz at the New York University (NYU) Courant Institute of Mathematical Sciences in the late 1960s.\n",
    "    \n",
    "    3. SETL provides two basic aggregate data types: unordered sets, and sequences (the latter also called tuples).\n",
    "    \n",
    "    4. The elements of sets and tuples can be of any arbitrary type, including sets and tuples themselves. Maps are provided as sets of pairs (i.e., tuples of length 2) and can have arbitrary domain and range types.\n",
    "    \n",
    "    5. Primitive operations in SETL include set membership, union, intersection, and power set construction, among others.\n",
    "    \n",
    "    6. SETL provides quantified boolean expressions constructed using the universal and existential quantifiers of first-order predicate logic.\n",
    "    \n",
    "    7. SETL provides several iterators to produce a variety of loops over aggregate data structures.\n",
    "    \n",
    "    8. SETL is an interpreted language with a syntax that is resembles C and in many cases similar to Perl.\n",
    "    \n",
    "    9. In SETL every statement is terminated by a semicolon.\n",
    "    \n",
    "    10. Variable names are case-insensitive and are automatically determined by their last assignment.\n",
    "    \n",
    "    11. Example:\n",
    "    \n",
    "        Print all prime numbers from 2 to N:\n",
    "\n",
    "        print([n in [2..N] | forall m in {2..n - 1} | n mod m > 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING WORKSHEET-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 to Q12 have only one correct answer. Choose the correct option to answer your question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. What is the most appropriate no. of clusters for the data points represented by the following dendrogram:\n",
    "    a) 2\n",
    "    b) 4\n",
    "    c) 6\n",
    "    d) 8\n",
    "\n",
    "--> b) 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. In which of the following cases will K-Means clustering fail to give good results?\n",
    "\n",
    "\n",
    "    1. Data points with outliers\n",
    "    2. Data points with different densities\n",
    "    3. Data points with round shapes\n",
    "    4. Data points with non-convex shapes\n",
    "    \n",
    "     Options:\n",
    "    a) 1 and 2\n",
    "    b) 2 and 3\n",
    "    c) 2 and 4\n",
    "    d) 1, 2 and 4\n",
    "\n",
    "--> d) 1,2 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. The most important part of is selecting the variables on which clustering is based.\n",
    "    a) interpreting and profiling clusters\n",
    "    b) selecting a clustering procedure\n",
    "    c) assessing the validity of clustering\n",
    "    d) formulating the clustering problem\n",
    "\n",
    "--> d) formulating the clustering problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. The most commonly used measure of similarity is the or its square.\n",
    "    a) Euclidean distance\n",
    "    b) city-block distance\n",
    "    c) Chebyshev’s distance\n",
    "    d) Manhattan distance\n",
    "\n",
    "--> a) Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. is a clustering procedure where all objects start out in one giant cluster. Clusters are formed by dividing this cluster into smaller and smaller clusters.\n",
    "    a) Non-hierarchical clustering\n",
    "    b) Divisive clustering\n",
    "    c) Agglomerative clustering\n",
    "    d) K-means clustering\n",
    "\n",
    "--> b) Divisive clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Which of the following is required by K-means clustering?\n",
    "    a) Defined distance metric\n",
    "    b) Number of clusters\n",
    "    c) Initial guess as to cluster centroids\n",
    "    d) All answers are correct\n",
    "\n",
    "--> d) All answers are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7. The goal of clustering is to \n",
    "    a) Divide the data points into groups\n",
    "    b) Classify the data point into different classes\n",
    "    c) Predict the output values of input data points\n",
    "    d) All of the above\n",
    "\n",
    "--> a) Divide the data points into groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    8. Clustering is a \n",
    "    a) Supervised learning\n",
    "    b) Unsupervised learning\n",
    "    c) Reinforcement learning\n",
    "    d) None\n",
    "\n",
    "--> b) Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    9. Which of the following clustering algorithms suffers from the problem of convergence at local optima?\n",
    "    a) K- Means clustering\n",
    "    b) Hierarchical clustering\n",
    "    c) Diverse clustering\n",
    "    d) All of the above\n",
    "\n",
    "--> d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    10. Which version of the clustering algorithm is most sensitive to outliers?\n",
    "    a) K-means clustering algorithm\n",
    "    b) K-modes clustering algorithm\n",
    "    c) K-medians clustering algorithm\n",
    "    d) None\n",
    "\n",
    "--> a) K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    11. Which of the following is a bad characteristic of a dataset for clustering analysis\n",
    "    a) Data points with outliers\n",
    "    b) Data points with different densities\n",
    "    c) Data points with non-convex shapes\n",
    "    d) All of the above\n",
    "\n",
    "--> d) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    12. For clustering, we do not require \n",
    "    a) Labeled data\n",
    "    b) Unlabeled data\n",
    "    c) Numerical data\n",
    "    d) Categorical data\n",
    "\n",
    "--> a) Labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13 to Q15 are subjective answers type questions, Answers them in their own words briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    13. How is cluster analysis calculated?\n",
    "    \n",
    "-->\n",
    "\n",
    "    \n",
    "    1. The hierarchical cluster analysis follows three basic steps:-\n",
    "    \n",
    "        i) calculate the distances\n",
    "        \n",
    "        ii) link the clusters\n",
    "        \n",
    "        iii) choose a solution by selecting the right number of clusters\n",
    "        \n",
    "    2. First, we have to select the variables upon which we base our clusters. In the dialog window we add the math, reading, and writing tests to the list of variables.  Since we want to cluster cases we leave the rest of the tick marks on the default.\n",
    "    \n",
    "    3. In the dialog box Statistics we can specify whether we want to output the proximity matrix (these are the distances calculated in the first step of the analysis) and the predicted cluster membership of the cases in our observations. Again, we leave all settings on default.\n",
    "    \n",
    "    4. In the dialog box Plots we should add the Dendrogram. The Dendrogram will graphically show how the clusters are merged and allows us to identify what the appropriate number of clusters is.\n",
    "    \n",
    "    5. The dialog box Method… allows us to specify the distance measure and the clustering method.  First, we need to define the correct distance measure.  SPSS offers three large blocks of distance measures for interval (scale), counts (ordinal), and binary (nominal) data.\n",
    "    \n",
    "    6. For interval data, the most common is Square Euclidian Distance. It is based on the Euclidian Distance between two observations, which is the square root of the sum of squared distances. Since the Euclidian Distance is squared, it increases the importance of large distances, while weakening the importance of small distances.\n",
    "    \n",
    "    7. If we have ordinal data (counts) we can select between Chi-Square or a standardized Chi-Square called Phi-Square.  For binary data, the Squared Euclidean Distance is commonly used.\n",
    "    \n",
    "    8. Next, we have to choose the Cluster Method.  Typically, choices are between-groups linkage, nearest neighbor, furthest neighbor, and Ward’s method. Single linkage works best with long chains of clusters, while complete linkage works best with dense blobs of clusters.  Between-groups linkage works with both cluster types.  It is recommended is to use single linkage first.  Although single linkage tends to create chains of clusters, it helps in identifying outliers.  After excluding these outliers, we can move onto Ward’s method.  Ward’s method uses the F value (like in ANOVA) to maximize the significance of differences between clusters.\n",
    "    \n",
    "    9. A last consideration is standardization. If the variables have different scales and means we might want to standardize either to Z scores or by centering the scale. We can also transform the values to absolute values if we have a data set where this might be appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    14. How is cluster quality measured?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. Once clustering is done, how well the clustering has performed can be quantified by a number of metrics. Ideal clustering is characterised by minimal intra cluster distance and maximal inter cluster distance.\n",
    "    \n",
    "    2. To measure the quality of clustering results, there are two kinds of validity indices: external indices and internal indices.\n",
    "\n",
    "    3. An external index is a measure of agreement between two partitions where the first partition is the a priori known clustering structure, and the second results from the clustering procedure.\n",
    "\n",
    "    4. Internal indices are used to measure the goodness of a clustering structure without external information.\n",
    "\n",
    "    5. For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels).\n",
    "\n",
    "    6. For internal indices, we evaluate the results using quantities and features inherent in the data set. The optimal number of clusters is usually determined based on an internal validity index.\n",
    "    \n",
    "    7. Extrinsic Measures require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure.\n",
    "    \n",
    "    8. Intrinsic Measures does not require ground truth labels. Some of the clustering performance measures are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    15. What is cluster analysis and its types?\n",
    "    \n",
    "-->\n",
    "\n",
    "\n",
    "    1. Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
    "    \n",
    "    2. It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n",
    "    \n",
    "    3. Types of cluster analysis:-\n",
    "    \n",
    "    i) Hierarchical clustering: Also known as 'nesting clustering' as it also clusters to exist within bigger clusters to form a tree.\n",
    "        \n",
    "    ii) Partition clustering: Its simply a division of the set of data objects into non-overlapping clusters such that each objects is in exactly one subset.\n",
    "        \n",
    "    iii) Exclusive Clustering: They assign each value to a single cluster.\n",
    "        \n",
    "    iv) Overlapping Clustering: It is used to reflect the fact that an object can simultaneously belong to more than one group.\n",
    "        \n",
    "    v) Fuzzy clustering: Every objects belongs to every cluster with a membership weight that goes between 0:if it absolutely doesn't belong to cluster and 1:if it absolutely belongs to the cluster.\n",
    "        \n",
    "    vi) Complete clustering: It perform a hierarchical clustering using a set of dissimilarities on 'n' objects that are being clustered. They tend to find compact clusters of an approaximately equal diameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
